{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_mrc_ner2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMW+uba/xyH4N0tbPJOK65b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ee53472428ac4e229cb37f02e30cc99e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ea3075767eb8445bae8a1f2ff9248135",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1132db0673db4fd78927fcffff5bd556",
              "IPY_MODEL_cfd21f3df3794b4e87e7ca2ebdac53b1"
            ]
          }
        },
        "ea3075767eb8445bae8a1f2ff9248135": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1132db0673db4fd78927fcffff5bd556": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_232c931fbbdc44e8931696e786f2a213",
            "_dom_classes": [],
            "description": "Epoch: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_33335e2987784861b769eba6faf2f41c"
          }
        },
        "cfd21f3df3794b4e87e7ca2ebdac53b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c8a405e1dea447078b5c7db08d064f45",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [16:31&lt;00:00, 991.17s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e77a6157ee90462ba5d05b99c91cb37d"
          }
        },
        "232c931fbbdc44e8931696e786f2a213": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "33335e2987784861b769eba6faf2f41c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c8a405e1dea447078b5c7db08d064f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e77a6157ee90462ba5d05b99c91cb37d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77a6efa9bfc947bb9ead606b7f7a3771": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5d6c91faaeec4e17acf05c1aeb301b40",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ae8c1e18bcd24efb8da48f82d0ecf040",
              "IPY_MODEL_27ed41bedd4743ca9b895529d1ed01c3"
            ]
          }
        },
        "5d6c91faaeec4e17acf05c1aeb301b40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "1000px",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "ae8c1e18bcd24efb8da48f82d0ecf040": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2c34fe74695f4453b150a262f7f85252",
            "_dom_classes": [],
            "description": "Train Step(14450 / 14450) (Mean loss=0.10914) (loss=0.00111): 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 14450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 14450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0379b51d6c6c427cbd8c13f7d0486715"
          }
        },
        "27ed41bedd4743ca9b895529d1ed01c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_495709f5dd784dcf9f94dd732a9a354c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 14450/14450 [16:30&lt;00:00, 15.08it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_79f7c90df4a043f0b1f0878acfc6d21c"
          }
        },
        "2c34fe74695f4453b150a262f7f85252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0379b51d6c6c427cbd8c13f7d0486715": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "495709f5dd784dcf9f94dd732a9a354c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "79f7c90df4a043f0b1f0878acfc6d21c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SayhellotoAI/NLP-progress/blob/master/bert_mrc_ner2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LrC_Ix0BbC1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fm8TaRKYEqrK",
        "outputId": "75b3c10d-1aee-4182-d41b-d3e81fc929ca"
      },
      "source": [
        "gdrive_path = \"/drive/My Drive/colab/bert-ner\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NcKt1_jEquA"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/drive/My Drive/colab/bert-ner/mrc-for-flat-nested-ner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJ6PxkLaEqwi",
        "outputId": "cd20c4f1-25ae-4495-88a2-147ab36ad40a"
      },
      "source": [
        "!pip install future\n",
        "!pip install tensorboard\n",
        "!pip install transformers\n",
        "#!pip install transformers -i https://pypi.python.org/simple\n",
        "!pip install -r '/drive/My Drive/colab/bert-ner/mrc-for-flat-nested-ner/requirements.txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (0.16.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.18.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (50.3.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (3.12.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.17.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.33.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard) (0.35.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (0.4.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.0.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard) (2.0.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfSlYc8IEsYd"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import copy\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import sys\n",
        "import collections\n",
        "import os\n",
        "import unicodedata\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "from io import open\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.nn import CrossEntropyLoss, Dropout, Embedding, Softmax\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
        "from tqdm.autonotebook import tqdm, trange\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "\n",
        "max_seq_length = 512 #@param {type: \"integer\"}\n",
        "doc_stride = 128 #@param {type: \"integer\"}\n",
        "max_query_length = 96 #@param {type: \"integer\"}\n",
        "max_answer_length = 30 #@param {type: \"integer\"}\n",
        "n_best_size = 57 #@param {type: \"integer\"}\n",
        "\n",
        "train_batch_size = 2 #@param {type: \"integer\"}\n",
        "learning_rate = 5e-5 #@param {type:\"raw\"}\n",
        "warmup_proportion = 0.1 #@param {type:\"raw\"}\n",
        "num_train_epochs = 10 #@param {type:\"raw\"}\n",
        "\n",
        "max_grad_norm = 1.0 #@param {type:\"raw\"}\n",
        "adam_epsilon = 1e-6 #@param {type:\"raw\"}\n",
        "weight_decay = 0.01 #@param {type:\"raw\"}\n",
        "\n",
        "gdrive_path = \"/drive/My Drive/colab/bert-ner/bert_small\"\n",
        "output_dir = gdrive_path\n",
        "checkpoint = os.path.join(gdrive_path, \"bert_small_ckpt.bin\")\n",
        "model_config = os.path.join(gdrive_path, \"bert_small_NER.json\")\n",
        "train_file = os.path.join(gdrive_path, \"KorQuAD_v1.0_train.json\")\n",
        "vocab_file = os.path.join(gdrive_path, \"ko_vocab_32k.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "5GdTmsRtEsdQ"
      },
      "source": [
        "#@title BertTokenizer (더블클릭하여 내용 확인 가능합니다.)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def load_vocab(vocab_file):\n",
        "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "    vocab = collections.OrderedDict()\n",
        "    index = 0\n",
        "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
        "        while True:\n",
        "            token = reader.readline()\n",
        "            if not token:\n",
        "                break\n",
        "            token = token.strip()\n",
        "            vocab[token] = index\n",
        "            index += 1\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def whitespace_tokenize(text):\n",
        "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "\n",
        "class BertTokenizer(object):\n",
        "    \"\"\"Runs end-to-end tokenization: punctuation splitting + wordpiece\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_file, do_lower_case=False, max_len=None, do_basic_tokenize=True,\n",
        "                 never_split=(\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\",\"PS\",\"LC\",\"OG\",\"AF\",\"DT\",\"TI\",\"CV\",\"AM\",\"PT\",\"QT\",\"FD\",\"TR\",\"EV\",\"MT\",\"TM\"\n",
        ")):\n",
        "        \"\"\"Constructs a BertTokenizer.\n",
        "\n",
        "        Args:\n",
        "          vocab_file: Path to a one-wordpiece-per-line vocabulary file\n",
        "          do_lower_case: Whether to lower case the input\n",
        "                         Only has an effect when do_wordpiece_only=False\n",
        "          do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n",
        "          max_len: An artificial maximum length to truncate tokenized sequences to;\n",
        "                         Effective maximum length is always the minimum of this\n",
        "                         value (if specified) and the underlying BERT model's\n",
        "                         sequence length.\n",
        "          never_split: List of tokens which will never be split during tokenization.\n",
        "                         Only has an effect when do_wordpiece_only=False\n",
        "        \"\"\"\n",
        "        if not os.path.isfile(vocab_file):\n",
        "            raise ValueError(\n",
        "                \"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"\n",
        "                \"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(vocab_file))\n",
        "        self.vocab = load_vocab(vocab_file)\n",
        "        self.ids_to_tokens = collections.OrderedDict(\n",
        "            [(ids, tok) for tok, ids in self.vocab.items()])\n",
        "        self.do_basic_tokenize = do_basic_tokenize\n",
        "        if do_basic_tokenize:\n",
        "            self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case,\n",
        "                                                  never_split=never_split)\n",
        "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
        "        self.max_len = max_len if max_len is not None else int(1e12)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        split_tokens = []\n",
        "        if self.do_basic_tokenize:\n",
        "            for token in self.basic_tokenizer.tokenize(text):\n",
        "                for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
        "                    split_tokens.append(sub_token)\n",
        "        else:\n",
        "            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n",
        "        return split_tokens\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n",
        "        ids = []\n",
        "        for token in tokens:\n",
        "            ids.append(self.vocab[token])\n",
        "        if len(ids) > self.max_len:\n",
        "            logger.warning(\n",
        "                \"Token indices sequence length is longer than the specified maximum \"\n",
        "                \" sequence length for this BERT model ({} > {}). Running this\"\n",
        "                \" sequence through BERT will result in indexing errors\".format(len(ids), self.max_len)\n",
        "            )\n",
        "        return ids\n",
        "\n",
        "    def convert_ids_to_tokens(self, ids):\n",
        "        \"\"\"Converts a sequence of ids in wordpiece tokens using the vocab.\"\"\"\n",
        "        tokens = []\n",
        "        for i in ids:\n",
        "            tokens.append(self.ids_to_tokens[i])\n",
        "        return tokens\n",
        "\n",
        "\n",
        "class BasicTokenizer(object):\n",
        "    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 do_lower_case=True,\n",
        "                 never_split=(\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")):\n",
        "        \"\"\"Constructs a BasicTokenizer.\n",
        "\n",
        "        Args:\n",
        "          do_lower_case: Whether to lower case the input.\n",
        "        \"\"\"\n",
        "        self.do_lower_case = do_lower_case\n",
        "        self.never_split = never_split\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenizes a piece of text.\"\"\"\n",
        "        text = self._clean_text(text)\n",
        "        text = self._tokenize_chinese_chars(text)\n",
        "        orig_tokens = whitespace_tokenize(text)\n",
        "        split_tokens = []\n",
        "        for token in orig_tokens:\n",
        "            if self.do_lower_case and token not in self.never_split:\n",
        "                token = self._run_strip_accents(token)\n",
        "            token = token.lower()\n",
        "            split_tokens.extend(self._run_split_on_punc(token))\n",
        "\n",
        "        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
        "        return output_tokens\n",
        "\n",
        "    def _run_strip_accents(self, text):\n",
        "        \"\"\"Strips accents from a piece of text.\"\"\"\n",
        "        text = unicodedata.normalize(\"NFD\", text)\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cat = unicodedata.category(char)\n",
        "            if cat == \"Mn\":\n",
        "                continue\n",
        "            output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "    def _run_split_on_punc(self, text):\n",
        "        \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
        "        if text in self.never_split:\n",
        "            return [text]\n",
        "        chars = list(text)\n",
        "        i = 0\n",
        "        start_new_word = True\n",
        "        output = []\n",
        "        while i < len(chars):\n",
        "            char = chars[i]\n",
        "            if _is_punctuation(char):\n",
        "                output.append([char])\n",
        "                start_new_word = True\n",
        "            else:\n",
        "                if start_new_word:\n",
        "                    output.append([])\n",
        "                start_new_word = False\n",
        "                output[-1].append(char)\n",
        "            i += 1\n",
        "\n",
        "        return [\"\".join(x) for x in output]\n",
        "\n",
        "    def _tokenize_chinese_chars(self, text):\n",
        "        \"\"\"Adds whitespace around any CJK character.\"\"\"\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cp = ord(char)\n",
        "            if self._is_chinese_char(cp):\n",
        "                output.append(\" \")\n",
        "                output.append(char)\n",
        "                output.append(\" \")\n",
        "            else:\n",
        "                output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "    def _is_chinese_char(self, cp):\n",
        "        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
        "        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
        "        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
        "        #\n",
        "        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
        "        # despite its name. The modern Korean Hangul alphabet is a different block,\n",
        "        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
        "        # space-separated words, so they are not treated specially and handled\n",
        "        # like the all of the other languages.\n",
        "        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
        "                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
        "                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
        "                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
        "                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
        "                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
        "                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
        "                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _clean_text(self, text):\n",
        "        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cp = ord(char)\n",
        "            if cp == 0 or cp == 0xfffd or _is_control(char):\n",
        "                continue\n",
        "            if _is_whitespace(char):\n",
        "                output.append(\" \")\n",
        "            else:\n",
        "                output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "\n",
        "class WordpieceTokenizer(object):\n",
        "    \"\"\"Runs WordPiece tokenization.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=100):\n",
        "        self.vocab = vocab\n",
        "        self.unk_token = unk_token\n",
        "        self.max_input_chars_per_word = max_input_chars_per_word\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenizes a piece of text into its word pieces.\n",
        "\n",
        "        This uses a greedy longest-match-first algorithm to perform tokenization\n",
        "        using the given vocabulary.\n",
        "\n",
        "        For example:\n",
        "          input = \"unaffable\"\n",
        "          output = [\"un\", \"##aff\", \"##able\"]\n",
        "\n",
        "        Args:\n",
        "          text: A single token or whitespace separated tokens. This should have\n",
        "            already been passed through `BasicTokenizer`.\n",
        "\n",
        "        Returns:\n",
        "          A list of wordpiece tokens.\n",
        "        \"\"\"\n",
        "\n",
        "        output_tokens = []\n",
        "        for token in whitespace_tokenize(text):\n",
        "            chars = list(token)\n",
        "            if len(chars) > self.max_input_chars_per_word:\n",
        "                output_tokens.append(self.unk_token)\n",
        "                continue\n",
        "\n",
        "            is_bad = False\n",
        "            start = 0\n",
        "            sub_tokens = []\n",
        "            while start < len(chars):\n",
        "                end = len(chars)\n",
        "                cur_substr = None\n",
        "                while start < end:\n",
        "                    substr = \"\".join(chars[start:end])\n",
        "                    if start > 0:\n",
        "                        substr = \"##\" + substr\n",
        "                    if substr in self.vocab:\n",
        "                        cur_substr = substr\n",
        "                        break\n",
        "                    end -= 1\n",
        "                if cur_substr is None:\n",
        "                    is_bad = True\n",
        "                    break\n",
        "                sub_tokens.append(cur_substr)\n",
        "                start = end\n",
        "\n",
        "            if is_bad:\n",
        "                output_tokens.append(self.unk_token)\n",
        "            else:\n",
        "                output_tokens.extend(sub_tokens)\n",
        "        return output_tokens\n",
        "\n",
        "\n",
        "def _is_whitespace(char):\n",
        "    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
        "    # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
        "    # as whitespace since they are generally considered as such.\n",
        "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "        return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat == \"Zs\":\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _is_control(char):\n",
        "    \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
        "    # These are technically control characters but we count them as whitespace\n",
        "    # characters.\n",
        "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "        return False\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"C\"):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _is_punctuation(char):\n",
        "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
        "    cp = ord(char)\n",
        "    # We treat all non-letter/number ASCII as punctuation.\n",
        "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
        "    # Punctuation class but we treat them as punctuation anyways, for\n",
        "    # consistency.\n",
        "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
        "            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
        "        return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"P\"):\n",
        "        return True\n",
        "    return False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "We8rE9TVEsfc"
      },
      "source": [
        "#@title MRC2NER Input Dataset (더블클릭하여 내용 확인 가능합니다.)\n",
        "from torch.utils.data import Dataset\n",
        "class MRC2NERInput(object):\n",
        "    def __init__(self, tokens, query, entity_label, impossible=True, end_position=[], span_position=[], start_position=[], qas_id=\"0.0\"):\n",
        "        self.tokens = tokens\n",
        "        self.end_position = end_position\n",
        "        self.entity_label = entity_label\n",
        "        self.impossible = impossible\n",
        "        self.query = query\n",
        "        self.qas_id = qas_id\n",
        "        self.span_position = span_position\n",
        "        self.start_position = start_position\n",
        "\n",
        "labels = {\"PS\":\"사람은 실존인물, 신화속 인물, 게임/소설의 가상 캐릭터 등을 나타낸다.\",\n",
        "          \"LA\":\"지역/장소와 지형/지리 명칭 등을 모두 포함한다. 지역/장소는 장소 계열로 표시하고, 지형/지리 명칭은 지리 계열로 표시한다.\",\n",
        "          \"OG\":\"기관/단체 명칭을 나타내며, 경제기관, 교육기관, 군사기관, 미디어/방송기관, 스포츠기관, 예술기관, 의료기관, 종교기관, 과학기관, 도서관, 법률기관, 정치/정부기관, 음식 관련 기관, 숙박기관 등으로 세분화된다.\",\n",
        "          \"AF\":\"인공물을 나타낸다. 인공물이란 사람에 의해 창조된 대상물을 말하며 문화재, 건물, 악기, 도로, 무기, 운송수단, 작품명 등이 이에 해당한다. 유의할 점은 인공물 내부에 포함된 세부범주에 해당되지 않는 인공물은 태깅 대상에서 배제한다는 점이다.\",\n",
        "          \"DT\":\"날짜 표현을 나타낸다\",\n",
        "          \"TI\":\"시간 표현을 나타낸다\",\n",
        "          \"CV\":\"문명/문화와 관련된 용어들로 구성되어 있다\",\n",
        "          \"AM\":\"생물은 진핵생물 중에서 동물에 해당되는 것으로, 사람을 제외한 짐승을 대상으로 한다.\",\n",
        "          \"PT\":\"식물 명칭을 나타낸다.\",\n",
        "          \"QT\":\"수량 표현을 나타낸다.\",\n",
        "          \"FD\":\"학문 분야를 나타낸다. 세부 범주로는 인문학, 사회과학, 자연과학, 응용과학으로 구성되어 있다. 더불어 학문 분야는 학파를 포함한다. 과학, 사회과학, 의학, 예술, 철학 관련 학파 및 유파들은 학문분야의 세부 범주로 명시된다.\",\n",
        "          \"TR\":\"이론은 특정 이론, 법칙, 원리 등의 명칭을 의미한다. 이론은 명확히 기술될 수 있는 고유의 이론, 법칙, 원리만을 말하며, 일반적인 용어들은 해당하지 않는다.\",\n",
        "          \"EV\":\"특정 사건/사고 명칭을 나타낸다.\",\n",
        "          \"MT\":\"물질에 대한 표현을 나타낸다.\",\n",
        "          \"TM\":\"위에서 정의된 세부 개체명 이외의 개체명 용어를 포함한다. 색, 방향, 기후지역, 모양, 세포 명칭과 생물의 조직 및 기관, 증상/증세/질병, 약, 전자기기 등 \"\n",
        "}\n",
        "label_to_idx = {f:i for i, f in enumerate(labels)}\n",
        "class MRCNERDataset(Dataset):\n",
        "    def __init__(self, tokenizer, read_data, possible_only=True):\n",
        "        self.read_data = read_data\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_seq_length\n",
        "        self.possible_only = possible_only\n",
        "\n",
        "        alldata = []\n",
        "        for (example_index, text) in enumerate(self.read_data):\n",
        "            text = re.sub(':LC', ':LA', text)\n",
        "            x = tokenizer.tokenize(text)\n",
        "        \n",
        "            del_list = []\n",
        "            y = []\n",
        "            j = 0\n",
        "            onedata = []\n",
        "            for i in labels:\n",
        "                onedata.append(MRC2NERInput(tokens = text,query=labels[i],entity_label=i,impossible=True,end_position=[],span_position=[],start_position=[],qas_id=str(example_index)+\".\"+str(label_to_idx[i])))\n",
        "                #context, end_position=[], entity_label, impossible=True, query, span_position=[], start_position=[]\n",
        "\n",
        "            for i in range(len(x)):\n",
        "                if i < j:\n",
        "                    continue\n",
        "                if x[i] == \"<\":\n",
        "                    for j in range(i,len(x)):\n",
        "                        if x[j] == ':' and x[j+2]==\">\":\n",
        "                            label = x[j+1]\n",
        "                            del_list.append(i)\n",
        "                            del_list.extend(list(range(j,j+3)))\n",
        "                            break\n",
        "\n",
        "                    onedata[label_to_idx[label.upper()]].start_position.append(len(y))\n",
        "                    onedata[label_to_idx[label.upper()]].impossible = False\n",
        "                    y.append(1)\n",
        "\n",
        "                    for k in range(i+2,j):                        \n",
        "                        y.append(2)\n",
        "\n",
        "                    onedata[label_to_idx[label.upper()]].end_position.append(len(y)-1)\n",
        "                    onedata[label_to_idx[label.upper()]].span_position.append([onedata[label_to_idx[label.upper()]].start_position[-1],len(y)-1])    \n",
        "                    j = j+3\n",
        "                else:\n",
        "                    y.append(0)\n",
        "\n",
        "            del_list.sort(reverse=True)\n",
        "            for d in del_list:\n",
        "                del x[d]\n",
        "            \n",
        "            for i in range(len(labels)):\n",
        "                onedata[i].tokens = x\n",
        "\n",
        "            alldata = alldata + onedata\n",
        "        \n",
        "        ## if impossible\n",
        "        alldata = [\n",
        "            x for x in alldata if x.start_position\n",
        "        ]\n",
        "        self.alldata = alldata\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.alldata)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            item: int, idx\n",
        "        Returns:\n",
        "            tokens: tokens of query + context, [seq_len]\n",
        "            token_type_ids: token type ids, 0 for query, 1 for context, [seq_len]\n",
        "            start_labels: start labels of NER in tokens, [seq_len]\n",
        "            end_labels: end labels of NER in tokens, [seq_len]\n",
        "            label_mask: label mask, 1 for counting into loss, 0 for ignoring. [seq_len]\n",
        "            match_labels: match labels, [seq_len, seq_len]\n",
        "            sample_idx: sample id\n",
        "            label_idx: label id\n",
        "        \"\"\"\n",
        "        data = self.alldata[item]\n",
        "        tokenizer = self.tokenizer\n",
        "\n",
        "        query = tokenizer.tokenize(data.query)\n",
        "\n",
        "        x = [\"[CLS]\"] + query + [\"[SEP]\"] + data.tokens + [\"[SEP]\"]\n",
        "        new_start_positions = [start+2+len(query) for start in data.start_position]\n",
        "        new_end_positions = [end+2+len(query) for end in data.end_position]\n",
        "        \n",
        "        if len(x) > max_seq_length:\n",
        "            x = query_tokens[0:max_seq_length]\n",
        "            print(\"x exceed max_seq_length\",len(x))\n",
        "\n",
        "        id_tokens = tokenizer.convert_tokens_to_ids(x)\n",
        "        token_type_ids = [[0] * (len(query)+2)] + [[1] * (len(data.tokens)+1)]\n",
        "        token_type_ids = sum(token_type_ids, [])\n",
        "        \n",
        "        match_labels = torch.zeros([max_seq_length, max_seq_length], dtype=torch.long)\n",
        "        for start, end in zip(new_start_positions, new_end_positions):\n",
        "            if start >= max_seq_length or end >= max_seq_length:\n",
        "                continue\n",
        "            match_labels[start, end] = 1\n",
        "        \n",
        "        sample_idx, label_idx = data.qas_id.split(\".\")\n",
        "        sample_idx = torch.LongTensor([int(sample_idx)])\n",
        "        label_idx = torch.LongTensor([int(label_idx)])   \n",
        "\n",
        "        start_labels = [(1 if idx in new_start_positions else 0)\n",
        "                        for idx in range(len(id_tokens))]\n",
        "        end_labels = [(1 if idx in new_end_positions else 0)\n",
        "                      for idx in range(len(id_tokens))]\n",
        "\n",
        "        #print(x)\n",
        "        #print(id_tokens)\n",
        "        #print(token_type_ids)\n",
        "        #print(start_labels)\n",
        "        #print(end_labels)\n",
        "        #print(len(id_tokens),len(token_type_ids))\n",
        "        #print(len(start_labels),len(end_labels))\n",
        "\n",
        "        while len(id_tokens) < max_seq_length:\n",
        "            id_tokens.append(0)\n",
        "            token_type_ids.append(0)\n",
        "            start_labels.append(0)\n",
        "            end_labels.append(0)\n",
        "\n",
        "        \n",
        "        return [\n",
        "            torch.LongTensor(id_tokens),\n",
        "            torch.LongTensor(token_type_ids),\n",
        "            torch.LongTensor(start_labels),\n",
        "            torch.LongTensor(end_labels),\n",
        "            torch.LongTensor(token_type_ids),#start_label_mask\n",
        "            torch.LongTensor(token_type_ids),#end _label_mask\n",
        "            #match_labels,\n",
        "            sample_idx,\n",
        "            label_idx\n",
        "        ]\n",
        "        #tokens, token_type_ids, start_labels, end_labels, start_label_mask, end_label_mask, sample_idx, label_idx = batch\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "L10mfOmyEsiF"
      },
      "source": [
        "#@title BERT Optimizer & Scheduler (더블클릭하여 내용 확인 가능합니다.)\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"PyTorch optimization for BERT model.\"\"\"\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ConstantLRSchedule(LambdaLR):\n",
        "    \"\"\" Constant learning rate schedule.\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, last_epoch=-1):\n",
        "        super(ConstantLRSchedule, self).__init__(optimizer, lambda _: 1.0, last_epoch=last_epoch)\n",
        "\n",
        "\n",
        "class WarmupConstantSchedule(LambdaLR):\n",
        "    \"\"\" Linear warmup and then constant.\n",
        "        Linearly increases learning rate schedule from 0 to 1 over `warmup_steps` training steps.\n",
        "        Keeps learning rate schedule equal to 1. after warmup_steps.\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, warmup_steps, last_epoch=-1):\n",
        "        self.warmup_steps = warmup_steps\n",
        "        super(WarmupConstantSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
        "\n",
        "    def lr_lambda(self, step):\n",
        "        if step < self.warmup_steps:\n",
        "            return float(step) / float(max(1.0, self.warmup_steps))\n",
        "        return 1.\n",
        "\n",
        "\n",
        "class WarmupLinearSchedule(LambdaLR):\n",
        "    \"\"\" Linear warmup and then linear decay.\n",
        "        Linearly increases learning rate from 0 to 1 over `warmup_steps` training steps.\n",
        "        Linearly decreases learning rate from 1. to 0. over remaining `t_total - warmup_steps` steps.\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, warmup_steps, t_total, last_epoch=-1):\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.t_total = t_total\n",
        "        super(WarmupLinearSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
        "\n",
        "    def lr_lambda(self, step):\n",
        "        if step < self.warmup_steps:\n",
        "            return float(step) / float(max(1, self.warmup_steps))\n",
        "        return max(0.0, float(self.t_total - step) / float(max(1.0, self.t_total - self.warmup_steps)))\n",
        "\n",
        "\n",
        "class WarmupCosineSchedule(LambdaLR):\n",
        "    \"\"\" Linear warmup and then cosine decay.\n",
        "        Linearly increases learning rate from 0 to 1 over `warmup_steps` training steps.\n",
        "        Decreases learning rate from 1. to 0. over remaining `t_total - warmup_steps` steps following a cosine curve.\n",
        "        If `cycles` (default=0.5) is different from default, learning rate follows cosine function after warmup.\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, warmup_steps, t_total, cycles=.5, last_epoch=-1):\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.t_total = t_total\n",
        "        self.cycles = cycles\n",
        "        super(WarmupCosineSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
        "\n",
        "    def lr_lambda(self, step):\n",
        "        if step < self.warmup_steps:\n",
        "            return float(step) / float(max(1.0, self.warmup_steps))\n",
        "        # progress after warmup\n",
        "        progress = float(step - self.warmup_steps) / float(max(1, self.t_total - self.warmup_steps))\n",
        "        return max(0.0, 0.5 * (1. + math.cos(math.pi * float(self.cycles) * 2.0 * progress)))\n",
        "\n",
        "\n",
        "class WarmupCosineWithHardRestartsSchedule(LambdaLR):\n",
        "    \"\"\" Linear warmup and then cosine cycles with hard restarts.\n",
        "        Linearly increases learning rate from 0 to 1 over `warmup_steps` training steps.\n",
        "        If `cycles` (default=1.) is different from default, learning rate follows `cycles` times a cosine decaying\n",
        "        learning rate (with hard restarts).\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, warmup_steps, t_total, cycles=1., last_epoch=-1):\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.t_total = t_total\n",
        "        self.cycles = cycles\n",
        "        super(WarmupCosineWithHardRestartsSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
        "\n",
        "    def lr_lambda(self, step):\n",
        "        if step < self.warmup_steps:\n",
        "            return float(step) / float(max(1, self.warmup_steps))\n",
        "        # progress after warmup\n",
        "        progress = float(step - self.warmup_steps) / float(max(1, self.t_total - self.warmup_steps))\n",
        "        if progress >= 1.0:\n",
        "            return 0.0\n",
        "        return max(0.0, 0.5 * (1. + math.cos(math.pi * ((float(self.cycles) * progress) % 1.0))))\n",
        "\n",
        "\n",
        "\n",
        "class AdamW(Optimizer):\n",
        "    \"\"\" Implements Adam algorithm with weight decay fix.\n",
        "\n",
        "    Parameters:\n",
        "        lr (float): learning rate. Default 1e-3.\n",
        "        betas (tuple of 2 floats): Adams beta parameters (b1, b2). Default: (0.9, 0.999)\n",
        "        eps (float): Adams epsilon. Default: 1e-6\n",
        "        weight_decay (float): Weight decay. Default: 0.0\n",
        "        correct_bias (bool): can be set to False to avoid correcting bias in Adam (e.g. like in Bert TF repository). Default True.\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.0, correct_bias=True):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1]  < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[1]))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(eps))\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n",
        "                        correct_bias=correct_bias)\n",
        "        super(AdamW, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                # In-place operations to update the averages at the same time\n",
        "                exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)\n",
        "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "\n",
        "                step_size = group['lr']\n",
        "                if group['correct_bias']:  # No bias correction for Bert\n",
        "                    bias_correction1 = 1.0 - beta1 ** state['step']\n",
        "                    bias_correction2 = 1.0 - beta2 ** state['step']\n",
        "                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n",
        "\n",
        "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
        "\n",
        "                # Just adding the square of the weights to the loss function is *not*\n",
        "                # the correct way of using L2 regularization/weight decay with Adam,\n",
        "                # since that will interact with the m and v parameters in strange ways.\n",
        "                #\n",
        "                # Instead we want to decay the weights in a manner that doesn't interact\n",
        "                # with the m/v parameters. This is equivalent to adding the square\n",
        "                # of the weights to the loss with plain (non-momentum) SGD.\n",
        "                # Add weight decay at the end (fixed version)\n",
        "                if group['weight_decay'] > 0.0:\n",
        "                    p.data.add_(-group['lr'] * group['weight_decay'], p.data)\n",
        "\n",
        "        return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsEyYThWEskN",
        "cellView": "form"
      },
      "source": [
        "#@title BERT Model (더블클릭하여 내용 확인 가능합니다.)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "LayerNorm = nn.LayerNorm\n",
        "\n",
        "\n",
        "def Linear(i_dim, o_dim, bias=True):\n",
        "    m = nn.Linear(i_dim, o_dim, bias)\n",
        "    nn.init.normal_(m.weight, std=0.02)\n",
        "    if bias:\n",
        "        nn.init.constant_(m.bias, 0.)\n",
        "    return m\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\" Implementation of the gelu activation function currently in Google Bert repo (identical to OpenAI GPT).\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Attention, self).__init__()\n",
        "        self.num_attention_heads = config.num_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query =Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.o_proj = Linear(config.hidden_size, config.hidden_size)\n",
        "        self.dropout = Dropout(config.dropout_prob)\n",
        "\n",
        "        self.softmax = Softmax(dim=-1)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "        attention_probs = self.softmax(attention_scores)\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        attention_output = self.o_proj(context_layer)\n",
        "\n",
        "        return attention_output\n",
        "\n",
        "\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = Linear(config.hidden_size, config.ff_dim)\n",
        "        self.fc2 = Linear(config.ff_dim, config.hidden_size)\n",
        "        self.act_fn = ACT2FN[config.act_fn]\n",
        "        self.dropout = Dropout(config.dropout_prob)\n",
        "\n",
        "    def forward(self, input):\n",
        "        intermediate = self.fc1(input)\n",
        "        ff_out = self.dropout(self.fc2(self.act_fn(intermediate)))\n",
        "        return ff_out\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Block, self).__init__()\n",
        "        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.ffn = PositionWiseFeedForward(config)\n",
        "        self.attn = Attention(config)\n",
        "\n",
        "    def forward(self, x, attention_mask):\n",
        "        # Attention\n",
        "        h = x\n",
        "        x = self.attn(x, attention_mask)\n",
        "        x = h + x\n",
        "        x = self.attention_norm(x)\n",
        "\n",
        "        # FFN\n",
        "        h = x\n",
        "        x = self.ffn(x)\n",
        "        x = x + h\n",
        "        x = self.ffn_norm(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layer = nn.ModuleList()\n",
        "        for l in range(config.num_hidden_layers):\n",
        "            layer = Block(config)\n",
        "            self.layer.append(copy.deepcopy(layer))\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        all_encoder_layers = []\n",
        "        for layer_block in self.layer:\n",
        "            hidden_states = layer_block(hidden_states, attention_mask)\n",
        "            all_encoder_layers.append(hidden_states)\n",
        "\n",
        "        return all_encoder_layers\n",
        "\n",
        "\n",
        "class Config(object):\n",
        "    def __init__(self,\n",
        "                 vocab_size_or_config_json_file,\n",
        "                 act_fn=\"gelu\",\n",
        "                 hidden_size=768,\n",
        "                 num_hidden_layers=12,\n",
        "                 ff_dim=3072,\n",
        "                 num_heads=12,\n",
        "                 dropout_prob=0.1,\n",
        "                 max_position_embeddings=512,\n",
        "                 type_vocab_size=2,\n",
        "                 initializer_range=0.02,\n",
        "                 num_classes=2\n",
        "                 ):\n",
        "        if isinstance(vocab_size_or_config_json_file, str):\n",
        "            with open(vocab_size_or_config_json_file, \"r\", encoding='utf-8') as reader:\n",
        "                json_config = json.loads(reader.read())\n",
        "            for key, value in json_config.items():\n",
        "                self.__dict__[key] = value\n",
        "        elif isinstance(vocab_size_or_config_json_file, int):\n",
        "            self.vocab_size = vocab_size_or_config_json_file\n",
        "            self.act_fn = act_fn\n",
        "            self.hidden_size = hidden_size\n",
        "            self.num_hidden_layers = num_hidden_layers\n",
        "            self.ff_dim = ff_dim\n",
        "            self.num_heads = num_heads\n",
        "            self.dropout_prob = dropout_prob\n",
        "            self.max_position_embeddings = max_position_embeddings\n",
        "            self.type_vocab_size = type_vocab_size\n",
        "            self.initializer_range = initializer_range\n",
        "            self.num_classes = num_classes\n",
        "        else:\n",
        "            raise ValueError(\"First argument must be either a vocabulary size (int)\"\n",
        "                             \"or the path to a pretrained model config file (str)\")\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, json_object):\n",
        "        config = Config(vocab_size_or_config_json_file=-1)\n",
        "        for key, value in json_object.items():\n",
        "            config.__dict__[key] = value\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_json_file(cls, json_file):\n",
        "        with open(json_file, \"r\", encoding='utf-8') as reader:\n",
        "            text = reader.read()\n",
        "        return cls.from_dict(json.loads(text))\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.word_embeddings = Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n",
        "        self.position_embeddings = Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        self.LayerNorm = LayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = Dropout(config.dropout_prob)\n",
        "\n",
        "        self.init_weights(config)\n",
        "\n",
        "    def init_weights(self, config):\n",
        "        self.word_embeddings.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
        "        self.position_embeddings.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
        "        self.token_type_embeddings.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None):\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "\n",
        "        words_embeddings = self.word_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class PredictionHeadTransform(nn.Module):  #\n",
        "    def __init__(self, config):\n",
        "        super(PredictionHeadTransform, self).__init__()\n",
        "        self.dense = Linear(config.hidden_size, config.hidden_size)\n",
        "        self.act_fn = ACT2FN[config.act_fn]\n",
        "        self.LayerNorm = LayerNorm(config.hidden_size, eps=1e-12)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.act_fn(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class LMPredictionHead(nn.Module):  #\n",
        "    def __init__(self, config, embedding_weights):\n",
        "        super(LMPredictionHead, self).__init__()\n",
        "        self.transform = PredictionHeadTransform(config)\n",
        "        self.decoder = Linear(embedding_weights.size(1),\n",
        "                              embedding_weights.size(0),\n",
        "                              bias=False)\n",
        "        self.decoder.weight = embedding_weights\n",
        "        self.bias = nn.Parameter(torch.zeros(embedding_weights.size(0)))\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.transform(hidden_states)\n",
        "        hidden_states = self.decoder(hidden_states) + self.bias\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class PreTrainingHeads(nn.Module):  #\n",
        "    def __init__(self, config, embedding_weights):\n",
        "        super(PreTrainingHeads, self).__init__()\n",
        "        self.predictions = LMPredictionHead(config, embedding_weights)\n",
        "\n",
        "    def forward(self, sequence_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "\n",
        "        return prediction_scores\n",
        "\n",
        "\n",
        "class Pooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Pooler, self).__init__()\n",
        "        self.dense = Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Model, self).__init__()\n",
        "        self.embeddings = Embeddings(config)\n",
        "        self.encoder = Encoder(config)\n",
        "        self.pooler = Pooler(config)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
        "        #extended_attention_mask = extended_attention_mask.to(torch.float32)\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
        "        encoded_layers = self.encoder(embedding_output,\n",
        "                                      extended_attention_mask)\n",
        "\n",
        "        sequence_output = encoded_layers[-1]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        return sequence_output , pooled_output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGu9x0RHEsmg"
      },
      "source": [
        "@title MRC Model\n",
        "# encoding: utf-8\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from loss import *\n",
        "from torch.nn.modules import CrossEntropyLoss, BCEWithLogitsLoss\n",
        "\n",
        "class SingleLinearClassifier(nn.Module):\n",
        "    def __init__(self, hidden_size, num_label):\n",
        "        super(SingleLinearClassifier, self).__init__()\n",
        "        self.num_label = num_label\n",
        "        self.classifier = nn.Linear(hidden_size, num_label)\n",
        "\n",
        "    def forward(self, input_features):\n",
        "        features_output = self.classifier(input_features)\n",
        "        return features_output\n",
        "\n",
        "\n",
        "class MultiNonLinearClassifier(nn.Module):\n",
        "    def __init__(self, hidden_size, num_label, dropout_rate):\n",
        "        super(MultiNonLinearClassifier, self).__init__()\n",
        "        self.num_label = num_label\n",
        "        self.classifier1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.classifier2 = nn.Linear(hidden_size, num_label)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, input_features):\n",
        "        features_output1 = self.classifier1(input_features)\n",
        "        # features_output1 = F.relu(features_output1)\n",
        "        features_output1 = F.gelu(features_output1)\n",
        "        features_output1 = self.dropout(features_output1)\n",
        "        features_output2 = self.classifier2(features_output1)\n",
        "        return features_output2\n",
        "        \n",
        "class BertQueryNER(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertQueryNER, self).__init__()\n",
        "        self.loss_type = \"bce\"\n",
        "        self.bce_loss = BCEWithLogitsLoss(reduction=\"none\")\n",
        "        dice_smooth = 1e-8\n",
        "        self.dice_loss = DiceLoss(with_logits=True, smooth=dice_smooth)\n",
        "        self.bert = Model(config)\n",
        "\n",
        "        self.start_outputs = nn.Linear(config.hidden_size, 1)\n",
        "        self.end_outputs = nn.Linear(config.hidden_size, 1)\n",
        "        self.span_embedding = MultiNonLinearClassifier(config.hidden_size * 2, 1, 0.3)#config.mrc_dropout)\n",
        "\n",
        "        self.hidden_size = config.hidden_size\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None,start_positions=None,end_positions=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_ids: bert input tokens, tensor of shape [seq_len]\n",
        "            token_type_ids: 0 for query, 1 for context, tensor of shape [seq_len]\n",
        "            attention_mask: attention mask, tensor of shape [seq_len]\n",
        "        Returns:\n",
        "            start_logits: start/non-start probs of shape [seq_len]\n",
        "            end_logits: end/non-end probs of shape [seq_len]\n",
        "            match_logits: start-end-match probs of shape [seq_len, 1]\n",
        "        \"\"\"\n",
        "\n",
        "        bert_outputs = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
        "\n",
        "        sequence_heatmap = bert_outputs[0]  # [batch, seq_len, hidden]\n",
        "        batch_size, seq_len, hid_size = sequence_heatmap.size()\n",
        "\n",
        "        start_logits = self.start_outputs(sequence_heatmap).squeeze(-1)  # [batch, seq_len, 1]\n",
        "        end_logits = self.end_outputs(sequence_heatmap).squeeze(-1)  # [batch, seq_len, 1]\n",
        "\n",
        "        start_extend = sequence_heatmap.unsqueeze(2).expand(-1, -1, seq_len, -1)\n",
        "        end_extend = sequence_heatmap.unsqueeze(1).expand(-1, seq_len, -1, -1)\n",
        "        span_matrix = torch.cat([start_extend, end_extend], 3)\n",
        "        span_logits = self.span_embedding(span_matrix).squeeze(-1)\n",
        "\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            batch_size, seq_len = start_logits.size()\n",
        "\n",
        "            start_float_label_mask = attention_mask.view(-1).float()\n",
        "            end_float_label_mask = attention_mask.view(-1).float()\n",
        "            match_label_row_mask = start_label_mask.bool().unsqueeze(-1).expand(-1, -1, seq_len)\n",
        "            match_label_col_mask = end_label_mask.bool().unsqueeze(-2).expand(-1, seq_len, -1)\n",
        "            match_label_mask = match_label_row_mask & match_label_col_mask\n",
        "            match_label_mask = torch.triu(match_label_mask, 0)  # start should be less equal to end\n",
        "\n",
        "            float_match_label_mask = match_label_mask.view(batch_size, -1).float()\n",
        "            \n",
        "            if self.loss_type == \"bce\":\n",
        "                start_loss = self.bce_loss(start_logits.view(-1), start_labels.view(-1).float())\n",
        "                start_loss = (start_loss * start_float_label_mask).sum() / start_float_label_mask.sum()\n",
        "                end_loss = self.bce_loss(end_logits.view(-1), end_labels.view(-1).float())\n",
        "                end_loss = (end_loss * end_float_label_mask).sum() / end_float_label_mask.sum()\n",
        "                match_loss = self.bce_loss(span_logits.view(batch_size, -1), match_labels.view(batch_size, -1).float())\n",
        "                match_loss = match_loss * float_match_label_mask\n",
        "                match_loss = match_loss.sum() / (float_match_label_mask.sum() + 1e-10)\n",
        "\n",
        "            else:\n",
        "                start_loss = self.dice_loss(start_logits, start_labels.float(), start_float_label_mask)\n",
        "                end_loss = self.dice_loss(end_logits, end_labels.float(), end_float_label_mask)\n",
        "                match_loss = self.dice_loss(span_logits, match_labels.float(), float_match_label_mask)\n",
        "\n",
        "            return start_loss, end_loss, match_loss, start_logits, end_logits, span_logits\n",
        "        \n",
        "        return start_logits, end_logits, span_logits\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG2bmvbdi3o2",
        "outputId": "291ead1d-fe5e-40d4-bb83-d53c6ff99fe3"
      },
      "source": [
        "#import torch_xla.core.xla_model as xm\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
        "                    level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "#device = xm.xla_device()\n",
        "logger.info(\"device: {} n_gpu: {}\".format(device, n_gpu))\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if n_gpu > 0:\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11/23/2020 14:54:38 - INFO - __main__ -   device: cuda n_gpu: 1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auIsaInwRvqA"
      },
      "source": [
        "# Prepare Tokenizer\n",
        "tokenizer = BertTokenizer(vocab_file, max_len=max_seq_length, do_basic_tokenize=True)\n",
        "config = Config.from_json_file(model_config)\n",
        "\n",
        "f = open(\"/drive/My Drive/colab/bert-ner/dataset/EXOBRAIN_NE_CORPUS_10000.txt\",\"r\")\n",
        "all_data = f.readlines()\n",
        "\n",
        "m = int(len(all_data)*0.8)\n",
        "train_examples = all_data[:m]\n",
        "test_examples = all_data[m:]\n",
        "\n",
        "train_data = MRCNERDataset(tokenizer,train_examples)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
        "num_train_optimization_steps = int(len(train_data) / train_batch_size) * num_train_epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOlo73cNPZqR",
        "outputId": "f0967110-93d7-4be2-f084-67e77d012bdd"
      },
      "source": [
        "model = BertQueryNER(config)\n",
        "model.bert.load_state_dict(torch.load(checkpoint,map_location=torch.device(\"cpu\")))\n",
        "num_params = count_parameters(model)\n",
        "logger.info(\"Total Parameter: %d\" % num_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11/23/2020 14:54:54 - INFO - __main__ -   Total Parameter: 17867522\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4o8S3-ui0E0"
      },
      "source": [
        "model.to(device)\n",
        "model.train()\n",
        "global_step = 0\n",
        "epoch = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPhCJY2rPZs1",
        "outputId": "f092d5aa-2dc3-4961-d133-7d7f02d9e939"
      },
      "source": [
        "# Prepare optimizer\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm']\n",
        "optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': weight_decay},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                  lr=learning_rate,\n",
        "                  eps=adam_epsilon)\n",
        "scheduler = WarmupLinearSchedule(optimizer,\n",
        "                                 warmup_steps=num_train_optimization_steps*0.1,\n",
        "                                 t_total=num_train_optimization_steps)\n",
        "\n",
        "logger.info(\"***** Running training *****\")\n",
        "logger.info(\"  Num orig examples = %d\", len(train_examples))\n",
        "logger.info(\"  Num split examples = %d\", len(train_data))\n",
        "logger.info(\"  Batch size = %d\", train_batch_size)\n",
        "logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
        "num_train_step = num_train_optimization_steps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11/23/2020 14:55:08 - INFO - __main__ -   ***** Running training *****\n",
            "11/23/2020 14:55:08 - INFO - __main__ -     Num orig examples = 8000\n",
            "11/23/2020 14:55:08 - INFO - __main__ -     Num split examples = 14450\n",
            "11/23/2020 14:55:08 - INFO - __main__ -     Batch size = 1\n",
            "11/23/2020 14:55:08 - INFO - __main__ -     Num steps = 14450\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "ee53472428ac4e229cb37f02e30cc99e",
            "ea3075767eb8445bae8a1f2ff9248135",
            "1132db0673db4fd78927fcffff5bd556",
            "cfd21f3df3794b4e87e7ca2ebdac53b1",
            "232c931fbbdc44e8931696e786f2a213",
            "33335e2987784861b769eba6faf2f41c",
            "c8a405e1dea447078b5c7db08d064f45",
            "e77a6157ee90462ba5d05b99c91cb37d",
            "77a6efa9bfc947bb9ead606b7f7a3771",
            "5d6c91faaeec4e17acf05c1aeb301b40",
            "ae8c1e18bcd24efb8da48f82d0ecf040",
            "27ed41bedd4743ca9b895529d1ed01c3",
            "2c34fe74695f4453b150a262f7f85252",
            "0379b51d6c6c427cbd8c13f7d0486715",
            "495709f5dd784dcf9f94dd732a9a354c",
            "79f7c90df4a043f0b1f0878acfc6d21c"
          ]
        },
        "id": "Bog6lR-APZvG",
        "outputId": "f2a1c1d6-bb43-4ef5-ba05-223cc09945fb"
      },
      "source": [
        "for _ in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
        "    iter_bar = tqdm(train_dataloader, desc=\"Train Step(XX/XX) (Mean loss=X.X) (loss=X.X)\", leave=False, ncols=1000)\n",
        "    tr_step, total_loss, mean_loss = 0, 0., 0.\n",
        "    for step, batch in enumerate(iter_bar):\n",
        "        batch = tuple(t.to(device) for t in batch)  # multi-gpu does scattering it-self\n",
        "        tokens, token_type_ids, start_labels, end_labels, start_label_mask, end_label_mask, sample_idx, label_idx = batch\n",
        "\n",
        "        start_loss, end_loss, match_loss, start_logits, end_logits, span_logits = model(tokens, token_type_ids=token_type_ids, attention_mask=start_label_mask, start_positions = start_labels, end_positions = end_labels)\n",
        "        #(self, input_ids, token_type_ids=None, attention_mask=None,tags=None):\n",
        "        loss = 1.0 * start_loss + 1.0 * end_loss\n",
        "\n",
        "        if n_gpu > 1:\n",
        "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "        #if config.gradient_accumulation_steps > 1:\n",
        "        #    loss = loss / config.gradient_accumulation_steps\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        global_step += 1\n",
        "        tr_step += 1\n",
        "        total_loss += loss\n",
        "        mean_loss = total_loss / tr_step\n",
        "        iter_bar.set_description(\"Train Step(%d / %d) (Mean loss=%5.5f) (loss=%5.5f)\" % (global_step, num_train_step, mean_loss, loss.item()))\n",
        "            \n",
        "    logger.info(\"***** Saving file *****\")\n",
        "    model_checkpoint = \"bertmrc/korquad_%d_%d.bin\" % (epoch,mean_loss)\n",
        "    logger.info(model_checkpoint)\n",
        "    output_model_file = os.path.join(output_dir,model_checkpoint)\n",
        "    torch.save(model.state_dict(), output_model_file)\n",
        "    epoch += 1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee53472428ac4e229cb37f02e30cc99e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77a6efa9bfc947bb9ead606b7f7a3771",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Train Step(XX/XX) (Mean loss=X.X) (loss=X.X)', layout=Lay…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:160: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "11/23/2020 15:12:06 - INFO - __main__ -   ***** Saving file *****\n",
            "11/23/2020 15:12:06 - INFO - __main__ -   bertmrc/korquad_0_0.bin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqVNM0kRoHLw"
      },
      "source": [
        "checkpoint = os.path.join(gdrive_path, \"bertmrc/korquad_7_7.bin\")#I-tag code korquad2\n",
        "tokenizer = BertTokenizer(vocab_file, max_len=max_seq_length, do_basic_tokenize=True)\n",
        "config = Config.from_json_file(model_config)\n",
        "\n",
        "predict_batch_size = 16 #@param {type: \"integer\"}\n",
        "\n",
        "model = BertQueryNER(config)\n",
        "model.load_state_dict(torch.load(checkpoint))\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "num_params = count_parameters(model)\n",
        "logger.info(\"Total Parameter: %d\" % num_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLnXyJ9PPZxf"
      },
      "source": [
        "test_data = MRCNERDataset(tokenizer,test_examples)\n",
        "test_sampler = RandomSampler(train_data)\n",
        "\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=predict_batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGgTZxVcVxpB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W17MsGMzPZz4"
      },
      "source": [
        "logger.info(\"Start evaluating!\")\n",
        "\n",
        "for tokens, token_type_ids, start_labels, end_labels, start_label_mask, end_label_mask, sample_idx, label_idx in tqdm(test_dataloader, desc=\"Evaluating\", leave=False, ncols=1000):\n",
        "    tokens = tokens.to(device)\n",
        "    token_type_ids = token_type_ids.to(device)\n",
        "    start_label_mask = start_label_mask.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        batch_start_logits, batch_end_logits = model(tokens, token_type_ids=token_type_ids, attention_mask=start_label_mask)\n",
        "    for i, example_index in enumerate(token_type_ids):\n",
        "        start_logits, end_logits = batch_start_logits[i], batch_end_logits[i]\n",
        "        unique_id = i\n",
        "        all_results.append([start_logits, start_labels[i],end_logits,end_labels[i]])\n",
        "       \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLk0d0IsPZ4O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUPiLglVPZ6-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2t8eEXBPZ2L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "8rTCM8XcEspB",
        "outputId": "26b32dbc-8036-4e0a-cb2b-ff2fa4b8d0d2"
      },
      "source": [
        "#encoding: utf-8\n",
        "\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "from collections import namedtuple\n",
        "from typing import Dict\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from torch import Tensor\n",
        "from torch.nn.modules import CrossEntropyLoss, BCEWithLogitsLoss\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "from torch.optim import SGD\n",
        "\n",
        "\n",
        "from datasets.truncate_dataset import TruncateDataset\n",
        "from datasets.collate_functions import collate_to_max_length\n",
        "from metrics.query_span_f1 import QuerySpanF1\n",
        "\n",
        "from loss import *\n",
        "from utils.radom_seed import set_random_seed\n",
        "import logging\n",
        "\n",
        "set_random_seed(0)\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "class BertLabeling(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        args: argparse.Namespace\n",
        "    ):\n",
        "        \"\"\"Initialize a model, tokenizer and config.\"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Prepare Tokenizer\n",
        "        self.tokenizer = BertTokenizer(vocab_file, max_len=max_seq_length, do_basic_tokenize=True)\n",
        "        self.dataset = MRCNERDataset(self.tokenizer)\n",
        "        config = Config.from_json_file(model_config)\n",
        "        self.model = BertQueryNER(config)\n",
        "        self.model.bert.load_state_dict(torch.load(checkpoint,map_location=torch.device(\"cpu\")))\n",
        "        num_params = count_parameters(self.model)\n",
        "        logger.info(\"Total Parameter: %d\" % num_params)\n",
        "\n",
        "        # logging.info(str(self.model))\n",
        "        #logging.info(str(args.__dict__ if isinstance(args, argparse.ArgumentParser) else args))\n",
        "        # self.ce_loss = CrossEntropyLoss(reduction=\"none\")\n",
        "        #self.loss_type = args.loss_type\n",
        "        self.loss_type = \"bce\"\n",
        "        weight_start = 1.0\n",
        "        weight_end = 1.0\n",
        "        weight_span = 1.0\n",
        "        flat = True\n",
        "        optimizer=\"adamw\"\n",
        "        chinese = False\n",
        "        dice_smooth = 1e-8\n",
        "        span_loss_candidates = \"all\"\n",
        "        self.lr = 3e-5\n",
        "        self.max_length = 512\n",
        "        self.batch_size = 1\n",
        "\n",
        "        if self.loss_type == \"bce\":\n",
        "            self.bce_loss = BCEWithLogitsLoss(reduction=\"none\")\n",
        "        else:\n",
        "            self.dice_loss = DiceLoss(with_logits=True, smooth=dice_smooth)\n",
        "        \n",
        "        weight_sum = weight_start + weight_end + weight_span\n",
        "        self.weight_start = weight_start / weight_sum\n",
        "        self.weight_end = weight_end / weight_sum\n",
        "        self.weight_span = weight_span / weight_sum\n",
        "        self.flat_ner = flat\n",
        "        self.span_f1 = QuerySpanF1(flat=self.flat_ner)\n",
        "        self.chinese = chinese\n",
        "        self.optimizer = optimizer\n",
        "        self.span_loss_candidates = span_loss_candidates\n",
        "        self.weight_decay = 0.01\n",
        "        self.final_div_factor = 1e4\n",
        "        self.warmup_steps = 0\n",
        "        self.workers = 0\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": self.weight_decay,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "        if self.optimizer == \"adamw\":\n",
        "            optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                              betas=(0.9, 0.98),  # according to RoBERTa paper\n",
        "                              lr=self.lr,\n",
        "                              eps=adam_epsilon,)\n",
        "        else:\n",
        "            optimizer = SGD(optimizer_grouped_parameters, lr=self.lr, momentum=0.9)\n",
        "\n",
        "        num_train_optimization_steps = int(self.dataset.__len__() / self.batch_size) * num_train_epochs\n",
        "        scheduler = WarmupLinearSchedule(optimizer,\n",
        "                                 warmup_steps=num_train_optimization_steps*0.1,\n",
        "                                 t_total=num_train_optimization_steps)\n",
        "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        \"\"\"\"\"\"\n",
        "        return self.model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "\n",
        "    def compute_loss(self, start_logits, end_logits,\n",
        "                     start_labels, end_labels, start_label_mask, end_label_mask):\n",
        "        batch_size, seq_len = start_logits.size()\n",
        "\n",
        "        start_float_label_mask = start_label_mask.view(-1).float()\n",
        "        end_float_label_mask = end_label_mask.view(-1).float()\n",
        "\n",
        "        if self.loss_type == \"bce\":\n",
        "            start_loss = self.bce_loss(start_logits.view(-1), start_labels.view(-1).float())\n",
        "            start_loss = (start_loss * start_float_label_mask).sum() / start_float_label_mask.sum()\n",
        "            end_loss = self.bce_loss(end_logits.view(-1), end_labels.view(-1).float())\n",
        "            end_loss = (end_loss * end_float_label_mask).sum() / end_float_label_mask.sum()\n",
        "\n",
        "        else:\n",
        "            start_loss = self.dice_loss(start_logits, start_labels.float(), start_float_label_mask)\n",
        "            end_loss = self.dice_loss(end_logits, end_labels.float(), end_float_label_mask)\n",
        "\n",
        "        return start_loss, end_loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"\"\"\"\n",
        "        tf_board_logs = {\n",
        "            \"lr\": self.trainer.optimizers[0].param_groups[0]['lr']\n",
        "        }\n",
        "        tokens, token_type_ids, start_labels, end_labels, start_label_mask, end_label_mask, sample_idx, label_idx = batch\n",
        "\n",
        "        # num_tasks * [bsz, length, num_labels]\n",
        "        attention_mask = (tokens != 0).long()\n",
        "        start_logits, end_logits = self(tokens, attention_mask, token_type_ids)\n",
        "\n",
        "        start_loss, end_loss = self.compute_loss(start_logits=start_logits,\n",
        "                                                             end_logits=end_logits,\n",
        "                                                             start_labels=start_labels,\n",
        "                                                             end_labels=end_labels,\n",
        "                                                             start_label_mask=start_label_mask,\n",
        "                                                             end_label_mask=end_label_mask\n",
        "                                                             )\n",
        "\n",
        "        total_loss = self.weight_start * start_loss + self.weight_end * end_loss\n",
        "\n",
        "        tf_board_logs[f\"train_loss\"] = total_loss\n",
        "        tf_board_logs[f\"start_loss\"] = start_loss\n",
        "        tf_board_logs[f\"end_loss\"] = end_loss\n",
        "\n",
        "        return {'loss': total_loss, 'log': tf_board_logs}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"\"\"\"\n",
        "\n",
        "        output = {}\n",
        "\n",
        "        tokens, token_type_ids, start_labels, end_labels, start_label_mask, end_label_mask, sample_idx, label_idx = batch\n",
        "\n",
        "        attention_mask = (tokens != 0).long()\n",
        "        start_logits, end_logits = self(tokens, attention_mask, token_type_ids)\n",
        "\n",
        "        start_loss, end_loss = self.compute_loss(start_logits=start_logits,\n",
        "                                                             end_logits=end_logits,\n",
        "                                                             start_labels=start_labels,\n",
        "                                                             end_labels=end_labels,\n",
        "                                                             start_label_mask=start_label_mask,\n",
        "                                                             end_label_mask=end_label_mask\n",
        "                                                             )\n",
        "\n",
        "        total_loss = self.weight_start * start_loss + self.weight_end * end_loss\n",
        "\n",
        "        output[f\"val_loss\"] = total_loss\n",
        "        output[f\"start_loss\"] = start_loss\n",
        "        output[f\"end_loss\"] = end_loss\n",
        "\n",
        "        start_preds, end_preds = start_logits > 0, end_logits > 0\n",
        "        span_f1_stats = self.span_f1(start_preds=start_preds, end_preds=end_preds,\n",
        "                                     start_label_mask=start_label_mask, end_label_mask=end_label_mask)\n",
        "        output[\"span_f1_stats\"] = span_f1_stats\n",
        "\n",
        "        return output\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        \"\"\"\"\"\"\n",
        "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "        tensorboard_logs = {'val_loss': avg_loss}\n",
        "\n",
        "        all_counts = torch.stack([x[f'span_f1_stats'] for x in outputs]).sum(0)\n",
        "        span_tp, span_fp, span_fn = all_counts\n",
        "        span_recall = span_tp / (span_tp + span_fn + 1e-10)\n",
        "        span_precision = span_tp / (span_tp + span_fp + 1e-10)\n",
        "        span_f1 = span_precision * span_recall * 2 / (span_recall + span_precision + 1e-10)\n",
        "        tensorboard_logs[f\"span_precision\"] = span_precision\n",
        "        tensorboard_logs[f\"span_recall\"] = span_recall\n",
        "        tensorboard_logs[f\"span_f1\"] = span_f1\n",
        "\n",
        "        return {'val_loss': avg_loss, 'log': tensorboard_logs}\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"\"\"\"\n",
        "        return self.validation_step(batch, batch_idx)\n",
        "\n",
        "    def test_epoch_end(\n",
        "        self,\n",
        "        outputs\n",
        "    ) -> Dict[str, Dict[str, Tensor]]:\n",
        "        \"\"\"\"\"\"\n",
        "        return self.validation_epoch_end(outputs)\n",
        "\n",
        "    def train_dataloader(self) -> DataLoader:\n",
        "        return self.get_dataloader(\"train\")\n",
        "        # return self.get_dataloader(\"dev\", 100)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return self.get_dataloader(\"dev\")\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return self.get_dataloader(\"test\")\n",
        "        # return self.get_dataloader(\"dev\")\n",
        "\n",
        "    def get_dataloader(self, prefix=\"train\", limit: int = None) -> DataLoader:\n",
        "        \"\"\"get training dataloader\"\"\"\n",
        "        \"\"\"\n",
        "        load_mmap_dataset\n",
        "        \"\"\"\n",
        "\n",
        "        #if limit is not None:\n",
        "        #    dataset = TruncateDataset(dataset, limit)\n",
        "\n",
        "        \n",
        "\n",
        "        return dataloader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    model = BertLabeling(args)\n",
        "    dataloader = get_dataloader()\n",
        "    print(dataloader)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cd610fead455>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrKFNGIBEsrY"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi8GFnSyEsty"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CZnqLt7EswH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8LzlfHTEsyo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLSWK-6JEs1J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOWUXrkGEs3Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEaSJfJlEs5u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLfeurVZEs8A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddo0McdREs-k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}